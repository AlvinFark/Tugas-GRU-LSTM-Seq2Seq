{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vinzgoldwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import NMT_Model\n",
    "import nmt_data_utils\n",
    "import nmt_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news-commentary-v8.de-en.en',\n",
    "          'r',\n",
    "          encoding = 'utf-8') as f:\n",
    "    en = f.readlines()\n",
    "    \n",
    "with open('news-commentary-v8.de-en.de',\n",
    "          'r',\n",
    "          encoding = 'utf-8') as f:\n",
    "    de = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SAN FRANCISCO – It has never been easy to have a rational conversation about the value of gold.\\n', 'SAN FRANCISCO – Es war noch nie leicht, ein rationales Gespräch über den Wert von Gold zu führen.\\n') \n",
      "\n",
      "('Lately, with gold prices up more than 300% over the last decade, it is harder than ever.\\n', 'In letzter Zeit allerdings ist dies schwieriger denn je, ist doch der Goldpreis im letzten Jahrzehnt um über 300 Prozent angestiegen.\\n') \n",
      "\n",
      "('Just last December, fellow economists Martin Feldstein and Nouriel Roubini each penned op-eds bravely questioning bullish market sentiment, sensibly pointing out gold’s risks.\\n', 'Erst letzten Dezember verfassten meine Kollegen Martin Feldstein und Nouriel Roubini Kommentare, in denen sie mutig die vorherrschende optimistische Marktstimmung hinterfragten und sehr überlegt auf die Risiken des Goldes \\xa0hinwiesen.\\n') \n",
      "\n",
      "('Wouldn’t you know it?\\n', 'Und es kam, wie es kommen musste.\\n') \n",
      "\n",
      "('Since their articles appeared, the price of gold has moved up still further.\\n', 'Seit der Veröffentlichung ihrer Artikel ist der Goldpreis noch weiter gestiegen.\\n') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 5 sentence pairs. \n",
    "for line in zip(en[:5], de[:5]):\n",
    "    print(line, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary new lines. \n",
    "de = [line.strip() for line in de]\n",
    "en = [line.strip() for line in en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(49, 599),\n",
       " (48, 599),\n",
       " (46, 583),\n",
       " (47, 547),\n",
       " (43, 514),\n",
       " (44, 512),\n",
       " (45, 511),\n",
       " (41, 509),\n",
       " (40, 503),\n",
       " (42, 490),\n",
       " (39, 477),\n",
       " (38, 443),\n",
       " (37, 438),\n",
       " (36, 421),\n",
       " (34, 412),\n",
       " (33, 365),\n",
       " (32, 358),\n",
       " (31, 353),\n",
       " (35, 346),\n",
       " (30, 326),\n",
       " (28, 324),\n",
       " (27, 273),\n",
       " (25, 260),\n",
       " (29, 254),\n",
       " (26, 250),\n",
       " (24, 233),\n",
       " (23, 232),\n",
       " (22, 214),\n",
       " (21, 208)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will only use sentences of similar lengths in order to make training easier. \n",
    "len_en = [len(sent) for sent in en if 20 < len(sent) < 50]\n",
    "len_dist = Counter(len_en).most_common()\n",
    "len_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11554"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11554 sentences that contain betwenn 20 and 50 words.\n",
    "len(len_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_de = []\n",
    "_en = []\n",
    "for sent_de, sent_en in zip(de, en):\n",
    "    if 20 < len(sent_en) < 50:\n",
    "        _de.append(sent_de)\n",
    "        _en.append(sent_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 420 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# but we will not use all 150 000 sentences, only 5000 for the beginning.\n",
    "text = _en[:3000]\n",
    "language='english'\n",
    "lower=True\n",
    "words = []\n",
    "tokenized_text = []\n",
    "\n",
    "for line in text:\n",
    "    tokenized = nltk.word_tokenize(line, language=language)\n",
    "    if lower:\n",
    "        tokenized = [word.lower() for word in tokenized]\n",
    "    tokenized_text.append(tokenized)\n",
    "    for word in tokenized:\n",
    "        words.append(word)\n",
    "\n",
    "most_common = Counter(words).most_common()\n",
    "en_preprocessed = tokenized_text\n",
    "en_most_common = most_common\n",
    "\n",
    "text = _de[:3000]\n",
    "language='german'\n",
    "lower=True\n",
    "words = []\n",
    "tokenized_text = []\n",
    "\n",
    "for line in text:\n",
    "    tokenized = nltk.word_tokenize(line, language=language)\n",
    "    if lower:\n",
    "        tokenized = [word.lower() for word in tokenized]\n",
    "    tokenized_text.append(tokenized)\n",
    "    for word in tokenized:\n",
    "        words.append(word)\n",
    "\n",
    "most_common = Counter(words).most_common()\n",
    "de_preprocessed = tokenized_text\n",
    "de_most_common = most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_preprocessed), len(de_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some of the sentences there is not german or english counterpart, i.e. only an empy array []\n",
    "# therefore we will remove those sentence pairs.\n",
    "en_preprocessed_clean, de_preprocessed_clean = [], []\n",
    "\n",
    "for sent_en, sent_de in zip(en_preprocessed, de_preprocessed):\n",
    "    if sent_en != [] and sent_de != []:\n",
    "        en_preprocessed_clean.append(sent_en)\n",
    "        de_preprocessed_clean.append(sent_de)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 997)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_preprocessed_clean), len(de_preprocessed_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      " ['wouldn', '’', 't', 'you', 'know', 'it', '?']\n",
      "German:\n",
      " ['und', 'es', 'kam', ',', 'wie', 'es', 'kommen', 'musste', '.'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['since', 'then', ',', 'the', 'index', 'has', 'climbed', 'above', '10,000', '.']\n",
      "German:\n",
      " ['seit', 'damals', 'ist', 'er', 'auf', 'über', '10.000', 'punkte', 'gestiegen', '.'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['they', 'departed', 'pledging', 'to', 'revive', 'europe', \"'s\", 'growth', '.']\n",
      "German:\n",
      " ['mit', 'der', 'zusicherung', ',', 'das', 'wachstum', 'in', 'europa', 'wieder', 'zu', 'beleben', ',', 'gingen', 'sie', 'auseinander', '.'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['we', \"'ve\", 'heard', 'that', 'empty', 'promise', 'before', '.']\n",
      "German:\n",
      " ['dieses', 'leere', 'versprechen', 'haben', 'wir', 'schon', 'einmal', 'gehört', '.'] \n",
      "\n",
      "\n",
      "\n",
      "English:\n",
      " ['many', 'europeans', 'are', 'sick', 'of', 'british', 'vetoes', '.']\n",
      "German:\n",
      " ['viele', 'europäer', 'sind', 'die', 'britischen', 'vetos', 'leid', '.'] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e, d in zip(en_preprocessed_clean, de_preprocessed_clean[:5]):\n",
    "    print('English:\\n', e)\n",
    "    print('German:\\n', d, '\\n'*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('.', 868),\n",
       "  ('the', 331),\n",
       "  ('is', 263),\n",
       "  (',', 198),\n",
       "  ('to', 145),\n",
       "  ('this', 129),\n",
       "  ('a', 122),\n",
       "  ('but', 108),\n",
       "  ('of', 103),\n",
       "  ('not', 100),\n",
       "  ('are', 97),\n",
       "  ('in', 97),\n",
       "  ('it', 92),\n",
       "  ('be', 85),\n",
       "  ('?', 81)],\n",
       " 2013,\n",
       " 2444)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_most_common[:15], len(en_most_common), len(de_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create oyr lookup dicts for english and german, i.e. our vocab. \n",
    "# we will also include special tokens, later on used in the model. \n",
    "specials = [\"<unk>\", \"<s>\", \"</s>\", '<pad>']\n",
    "\n",
    "en_word2ind, en_ind2word, en_vocab_size = nmt_data_utils.create_vocab(en_most_common, specials)\n",
    "de_word2ind, de_ind2word, de_vocab_size = nmt_data_utils.create_vocab(de_most_common, specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to feed the sentences to the network, we have to convert them to ints, corresponding to their indices\n",
    "# in the lookup dicts. \n",
    "# we reverse the source language sentences, i.e. the english sentences as this alleviates learning for the seq2seq \n",
    "# model. Apart from this we also include EndOfSentence and StartOfSentence tags, which are needed as well. \n",
    "en_inds, en_unknowns = nmt_data_utils.convert_to_inds(en_preprocessed_clean, en_word2ind, reverse = True, eos = True)\n",
    "de_inds, de_unknowns = nmt_data_utils.convert_to_inds(de_preprocessed_clean, de_word2ind, sos = True, eos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['?', 'it', 'know', 'you', 't', '’', 'wouldn', '</s>'],\n",
       " ['.',\n",
       "  '10,000',\n",
       "  'above',\n",
       "  'climbed',\n",
       "  'has',\n",
       "  'index',\n",
       "  'the',\n",
       "  ',',\n",
       "  'then',\n",
       "  'since',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nmt_data_utils.convert_to_words(sentence, en_ind2word) for sentence in  en_inds[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  'und',\n",
       "  'es',\n",
       "  'kam',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'es',\n",
       "  'kommen',\n",
       "  'musste',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  'seit',\n",
       "  'damals',\n",
       "  'ist',\n",
       "  'er',\n",
       "  'auf',\n",
       "  'über',\n",
       "  '10.000',\n",
       "  'punkte',\n",
       "  'gestiegen',\n",
       "  '.',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nmt_data_utils.convert_to_words(sentence, de_ind2word) for sentence in  de_inds[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams. \n",
    "# those are probably not perfect, but work fine for now. \n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 128\n",
    "rnn_size_decoder = 128\n",
    "embedding_dim = 300\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "clip = 5\n",
    "keep_probability = 0.8\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay_steps = 1000\n",
    "learning_rate_decay = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\New folder\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\GRU\\NMT_Model.py:106: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\GRU\\NMT_Model.py:181: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\New folder\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\New folder\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\New folder\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\vinzgoldwin\\GRU\\NMT_Model.py:331: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Graph built.\n",
      "-------------------- Epoch 0 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 7.8032\n",
      "Iteration: 2 of 15\ttrain_loss: 7.4550\n",
      "Iteration: 4 of 15\ttrain_loss: 6.0873\n",
      "Iteration: 6 of 15\ttrain_loss: 5.9519\n",
      "Iteration: 8 of 15\ttrain_loss: 5.8739\n",
      "Iteration: 10 of 15\ttrain_loss: 5.9788\n",
      "Iteration: 12 of 15\ttrain_loss: 5.8814\n",
      "Iteration: 14 of 15\ttrain_loss: 5.9532\n",
      "Iteration: 15 of 15\ttrain_loss: 5.9122\n",
      "Average Score for this Epoch: 6.322536468505859\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 1 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 5.1883\n",
      "Iteration: 2 of 15\ttrain_loss: 5.3058\n",
      "Iteration: 4 of 15\ttrain_loss: 5.4531\n",
      "Iteration: 6 of 15\ttrain_loss: 5.3952\n",
      "Iteration: 8 of 15\ttrain_loss: 5.4303\n",
      "Iteration: 10 of 15\ttrain_loss: 5.5004\n",
      "Iteration: 12 of 15\ttrain_loss: 5.4625\n",
      "Iteration: 14 of 15\ttrain_loss: 5.4161\n",
      "Iteration: 15 of 15\ttrain_loss: 5.2175\n",
      "Average Score for this Epoch: 5.400977611541748\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 2 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 5.0710\n",
      "Iteration: 2 of 15\ttrain_loss: 5.1080\n",
      "Iteration: 4 of 15\ttrain_loss: 5.0838\n",
      "Iteration: 6 of 15\ttrain_loss: 5.0242\n",
      "Iteration: 8 of 15\ttrain_loss: 5.2279\n",
      "Iteration: 10 of 15\ttrain_loss: 5.2809\n",
      "Iteration: 12 of 15\ttrain_loss: 5.3323\n",
      "Iteration: 14 of 15\ttrain_loss: 5.2120\n",
      "Iteration: 15 of 15\ttrain_loss: 5.1161\n",
      "Average Score for this Epoch: 5.165650367736816\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 3 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.8537\n",
      "Iteration: 2 of 15\ttrain_loss: 4.9020\n",
      "Iteration: 4 of 15\ttrain_loss: 4.9868\n",
      "Iteration: 6 of 15\ttrain_loss: 5.0796\n",
      "Iteration: 8 of 15\ttrain_loss: 4.9440\n",
      "Iteration: 10 of 15\ttrain_loss: 5.0767\n",
      "Iteration: 12 of 15\ttrain_loss: 5.1482\n",
      "Iteration: 14 of 15\ttrain_loss: 5.0921\n",
      "Iteration: 15 of 15\ttrain_loss: 5.0211\n",
      "Average Score for this Epoch: 5.017592906951904\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 4 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.6935\n",
      "Iteration: 2 of 15\ttrain_loss: 4.8540\n",
      "Iteration: 4 of 15\ttrain_loss: 4.8186\n",
      "Iteration: 6 of 15\ttrain_loss: 4.8986\n",
      "Iteration: 8 of 15\ttrain_loss: 4.8576\n",
      "Iteration: 10 of 15\ttrain_loss: 4.8976\n",
      "Iteration: 12 of 15\ttrain_loss: 5.0553\n",
      "Iteration: 14 of 15\ttrain_loss: 5.0668\n",
      "Iteration: 15 of 15\ttrain_loss: 4.7581\n",
      "Average Score for this Epoch: 4.887434005737305\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 5 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.5847\n",
      "Iteration: 2 of 15\ttrain_loss: 4.6528\n",
      "Iteration: 4 of 15\ttrain_loss: 4.6280\n",
      "Iteration: 6 of 15\ttrain_loss: 4.7921\n",
      "Iteration: 8 of 15\ttrain_loss: 4.7953\n",
      "Iteration: 10 of 15\ttrain_loss: 4.7835\n",
      "Iteration: 12 of 15\ttrain_loss: 4.8789\n",
      "Iteration: 14 of 15\ttrain_loss: 5.0578\n",
      "Iteration: 15 of 15\ttrain_loss: 4.6564\n",
      "Average Score for this Epoch: 4.770543098449707\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 6 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.5091\n",
      "Iteration: 2 of 15\ttrain_loss: 4.5288\n",
      "Iteration: 4 of 15\ttrain_loss: 4.5043\n",
      "Iteration: 6 of 15\ttrain_loss: 4.6104\n",
      "Iteration: 8 of 15\ttrain_loss: 4.7154\n",
      "Iteration: 10 of 15\ttrain_loss: 4.8005\n",
      "Iteration: 12 of 15\ttrain_loss: 4.7865\n",
      "Iteration: 14 of 15\ttrain_loss: 4.7597\n",
      "Iteration: 15 of 15\ttrain_loss: 4.6957\n",
      "Average Score for this Epoch: 4.656824111938477\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 7 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.5205\n",
      "Iteration: 2 of 15\ttrain_loss: 4.4546\n",
      "Iteration: 4 of 15\ttrain_loss: 4.4704\n",
      "Iteration: 6 of 15\ttrain_loss: 4.5038\n",
      "Iteration: 8 of 15\ttrain_loss: 4.4916\n",
      "Iteration: 10 of 15\ttrain_loss: 4.4590\n",
      "Iteration: 12 of 15\ttrain_loss: 4.8193\n",
      "Iteration: 14 of 15\ttrain_loss: 4.7287\n",
      "Iteration: 15 of 15\ttrain_loss: 4.8080\n",
      "Average Score for this Epoch: 4.558060646057129\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 8 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.2519\n",
      "Iteration: 2 of 15\ttrain_loss: 4.0906\n",
      "Iteration: 4 of 15\ttrain_loss: 4.4457\n",
      "Iteration: 6 of 15\ttrain_loss: 4.4691\n",
      "Iteration: 8 of 15\ttrain_loss: 4.4866\n",
      "Iteration: 10 of 15\ttrain_loss: 4.4502\n",
      "Iteration: 12 of 15\ttrain_loss: 4.5517\n",
      "Iteration: 14 of 15\ttrain_loss: 4.5787\n",
      "Iteration: 15 of 15\ttrain_loss: 4.4831\n",
      "Average Score for this Epoch: 4.436540603637695\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 9 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.0969\n",
      "Iteration: 2 of 15\ttrain_loss: 4.1269\n",
      "Iteration: 4 of 15\ttrain_loss: 4.3038\n",
      "Iteration: 6 of 15\ttrain_loss: 4.1629\n",
      "Iteration: 8 of 15\ttrain_loss: 4.4765\n",
      "Iteration: 10 of 15\ttrain_loss: 4.3025\n",
      "Iteration: 12 of 15\ttrain_loss: 4.3771\n",
      "Iteration: 14 of 15\ttrain_loss: 4.5154\n",
      "Iteration: 15 of 15\ttrain_loss: 4.1840\n",
      "Average Score for this Epoch: 4.317474365234375\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 10 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.9157\n",
      "Iteration: 2 of 15\ttrain_loss: 4.0350\n",
      "Iteration: 4 of 15\ttrain_loss: 4.0641\n",
      "Iteration: 6 of 15\ttrain_loss: 4.0226\n",
      "Iteration: 8 of 15\ttrain_loss: 4.2858\n",
      "Iteration: 10 of 15\ttrain_loss: 4.3466\n",
      "Iteration: 12 of 15\ttrain_loss: 4.2594\n",
      "Iteration: 14 of 15\ttrain_loss: 4.1502\n",
      "Iteration: 15 of 15\ttrain_loss: 4.2054\n",
      "Average Score for this Epoch: 4.195293426513672\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 11 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 4.1234\n",
      "Iteration: 2 of 15\ttrain_loss: 3.9501\n",
      "Iteration: 4 of 15\ttrain_loss: 3.9992\n",
      "Iteration: 6 of 15\ttrain_loss: 4.1379\n",
      "Iteration: 8 of 15\ttrain_loss: 4.0389\n",
      "Iteration: 10 of 15\ttrain_loss: 4.1874\n",
      "Iteration: 12 of 15\ttrain_loss: 4.2437\n",
      "Iteration: 14 of 15\ttrain_loss: 4.2827\n",
      "Iteration: 15 of 15\ttrain_loss: 4.1196\n",
      "Average Score for this Epoch: 4.089419364929199\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 12 of 150 --------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 of 15\ttrain_loss: 3.7786\n",
      "Iteration: 2 of 15\ttrain_loss: 3.9165\n",
      "Iteration: 4 of 15\ttrain_loss: 3.8842\n",
      "Iteration: 6 of 15\ttrain_loss: 4.1170\n",
      "Iteration: 8 of 15\ttrain_loss: 4.0415\n",
      "Iteration: 10 of 15\ttrain_loss: 4.0676\n",
      "Iteration: 12 of 15\ttrain_loss: 4.0868\n",
      "Iteration: 14 of 15\ttrain_loss: 4.0057\n",
      "Iteration: 15 of 15\ttrain_loss: 3.7603\n",
      "Average Score for this Epoch: 3.9627456665039062\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 13 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.8823\n",
      "Iteration: 2 of 15\ttrain_loss: 3.7612\n",
      "Iteration: 4 of 15\ttrain_loss: 3.9176\n",
      "Iteration: 6 of 15\ttrain_loss: 3.7326\n",
      "Iteration: 8 of 15\ttrain_loss: 3.9204\n",
      "Iteration: 10 of 15\ttrain_loss: 3.8732\n",
      "Iteration: 12 of 15\ttrain_loss: 3.9084\n",
      "Iteration: 14 of 15\ttrain_loss: 4.0497\n",
      "Iteration: 15 of 15\ttrain_loss: 4.0004\n",
      "Average Score for this Epoch: 3.8537750244140625\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 14 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.5590\n",
      "Iteration: 2 of 15\ttrain_loss: 3.5239\n",
      "Iteration: 4 of 15\ttrain_loss: 3.6411\n",
      "Iteration: 6 of 15\ttrain_loss: 3.7247\n",
      "Iteration: 8 of 15\ttrain_loss: 3.7116\n",
      "Iteration: 10 of 15\ttrain_loss: 3.8980\n",
      "Iteration: 12 of 15\ttrain_loss: 3.8939\n",
      "Iteration: 14 of 15\ttrain_loss: 3.7650\n",
      "Iteration: 15 of 15\ttrain_loss: 3.6323\n",
      "Average Score for this Epoch: 3.722139835357666\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 15 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.5129\n",
      "Iteration: 2 of 15\ttrain_loss: 3.6192\n",
      "Iteration: 4 of 15\ttrain_loss: 3.5454\n",
      "Iteration: 6 of 15\ttrain_loss: 3.6214\n",
      "Iteration: 8 of 15\ttrain_loss: 3.5571\n",
      "Iteration: 10 of 15\ttrain_loss: 3.4836\n",
      "Iteration: 12 of 15\ttrain_loss: 3.7100\n",
      "Iteration: 14 of 15\ttrain_loss: 3.6702\n",
      "Iteration: 15 of 15\ttrain_loss: 3.7612\n",
      "Average Score for this Epoch: 3.608823776245117\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 16 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.3784\n",
      "Iteration: 2 of 15\ttrain_loss: 3.3108\n",
      "Iteration: 4 of 15\ttrain_loss: 3.3904\n",
      "Iteration: 6 of 15\ttrain_loss: 3.4757\n",
      "Iteration: 8 of 15\ttrain_loss: 3.5181\n",
      "Iteration: 10 of 15\ttrain_loss: 3.6843\n",
      "Iteration: 12 of 15\ttrain_loss: 3.5528\n",
      "Iteration: 14 of 15\ttrain_loss: 3.6488\n",
      "Iteration: 15 of 15\ttrain_loss: 3.4150\n",
      "Average Score for this Epoch: 3.497690200805664\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 17 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.2861\n",
      "Iteration: 2 of 15\ttrain_loss: 3.3338\n",
      "Iteration: 4 of 15\ttrain_loss: 3.2223\n",
      "Iteration: 6 of 15\ttrain_loss: 3.3701\n",
      "Iteration: 8 of 15\ttrain_loss: 3.4448\n",
      "Iteration: 10 of 15\ttrain_loss: 3.3453\n",
      "Iteration: 12 of 15\ttrain_loss: 3.6129\n",
      "Iteration: 14 of 15\ttrain_loss: 3.6279\n",
      "Iteration: 15 of 15\ttrain_loss: 3.3378\n",
      "Average Score for this Epoch: 3.386255979537964\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 18 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.0027\n",
      "Iteration: 2 of 15\ttrain_loss: 3.1478\n",
      "Iteration: 4 of 15\ttrain_loss: 3.2924\n",
      "Iteration: 6 of 15\ttrain_loss: 3.4983\n",
      "Iteration: 8 of 15\ttrain_loss: 3.3329\n",
      "Iteration: 10 of 15\ttrain_loss: 3.2443\n",
      "Iteration: 12 of 15\ttrain_loss: 3.3885\n",
      "Iteration: 14 of 15\ttrain_loss: 3.5527\n",
      "Iteration: 15 of 15\ttrain_loss: 3.1746\n",
      "Average Score for this Epoch: 3.2689242362976074\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 19 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 3.0786\n",
      "Iteration: 2 of 15\ttrain_loss: 3.1977\n",
      "Iteration: 4 of 15\ttrain_loss: 3.0856\n",
      "Iteration: 6 of 15\ttrain_loss: 3.0521\n",
      "Iteration: 8 of 15\ttrain_loss: 3.2947\n",
      "Iteration: 10 of 15\ttrain_loss: 3.2195\n",
      "Iteration: 12 of 15\ttrain_loss: 3.2889\n",
      "Iteration: 14 of 15\ttrain_loss: 3.1975\n",
      "Iteration: 15 of 15\ttrain_loss: 3.1624\n",
      "Average Score for this Epoch: 3.1625890731811523\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 20 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 2.9759\n",
      "Iteration: 2 of 15\ttrain_loss: 3.0945\n",
      "Iteration: 4 of 15\ttrain_loss: 3.1154\n",
      "Iteration: 6 of 15\ttrain_loss: 3.0809\n",
      "Iteration: 8 of 15\ttrain_loss: 2.9386\n",
      "Iteration: 10 of 15\ttrain_loss: 3.1396\n",
      "Iteration: 12 of 15\ttrain_loss: 3.1299\n",
      "Iteration: 14 of 15\ttrain_loss: 3.1798\n",
      "Iteration: 15 of 15\ttrain_loss: 3.1443\n",
      "Average Score for this Epoch: 3.064694404602051\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 21 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 2.8977\n",
      "Iteration: 2 of 15\ttrain_loss: 2.8658\n",
      "Iteration: 4 of 15\ttrain_loss: 2.8588\n",
      "Iteration: 6 of 15\ttrain_loss: 2.8586\n",
      "Iteration: 8 of 15\ttrain_loss: 2.9431\n",
      "Iteration: 10 of 15\ttrain_loss: 2.9684\n",
      "Iteration: 12 of 15\ttrain_loss: 3.3445\n",
      "Iteration: 14 of 15\ttrain_loss: 3.1565\n",
      "Iteration: 15 of 15\ttrain_loss: 2.9952\n",
      "Average Score for this Epoch: 2.9563465118408203\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 22 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 2.7686\n",
      "Iteration: 2 of 15\ttrain_loss: 2.7628\n",
      "Iteration: 4 of 15\ttrain_loss: 2.6522\n",
      "Iteration: 6 of 15\ttrain_loss: 2.9019\n",
      "Iteration: 8 of 15\ttrain_loss: 2.8796\n",
      "Iteration: 10 of 15\ttrain_loss: 2.9617\n",
      "Iteration: 12 of 15\ttrain_loss: 3.0188\n",
      "Iteration: 14 of 15\ttrain_loss: 3.1823\n",
      "Iteration: 15 of 15\ttrain_loss: 2.7467\n",
      "Average Score for this Epoch: 2.840717315673828\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 23 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 2.7446\n",
      "Iteration: 2 of 15\ttrain_loss: 2.6456\n",
      "Iteration: 4 of 15\ttrain_loss: 2.6878\n",
      "Iteration: 6 of 15\ttrain_loss: 2.7095\n",
      "Iteration: 8 of 15\ttrain_loss: 2.7414\n",
      "Iteration: 10 of 15\ttrain_loss: 3.0060\n",
      "Iteration: 12 of 15\ttrain_loss: 2.8234\n",
      "Iteration: 14 of 15\ttrain_loss: 2.7942\n",
      "Iteration: 15 of 15\ttrain_loss: 2.9247\n",
      "Average Score for this Epoch: 2.744140625\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 24 of 150 --------------------\n",
      "Iteration: 0 of 15\ttrain_loss: 2.5407\n",
      "Iteration: 2 of 15\ttrain_loss: 2.4853\n"
     ]
    }
   ],
   "source": [
    "# create the graph and train the model. \n",
    "nmt_model_utils.reset_graph()\n",
    "\n",
    "nmt = NMT_Model.NMT(en_word2ind,\n",
    "                    en_ind2word,\n",
    "                    de_word2ind,\n",
    "                    de_ind2word,\n",
    "                    './models/local_one/my_model',\n",
    "                    'TRAIN',\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    num_layers_encoder = num_layers_encoder,\n",
    "                    num_layers_decoder = num_layers_decoder,\n",
    "                    batch_size = batch_size,\n",
    "                    clip = clip,\n",
    "                    keep_probability = keep_probability,\n",
    "                    learning_rate = learning_rate,\n",
    "                    epochs = epochs,\n",
    "                    rnn_size_encoder = rnn_size_encoder,\n",
    "                    rnn_size_decoder = rnn_size_decoder, \n",
    "                    learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                    learning_rate_decay = learning_rate_decay)\n",
    "  \n",
    "nmt.build_graph()\n",
    "nmt.train(en_inds, de_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_de_inds, _de_unknowns = nmt_data_utils.convert_to_inds(de_preprocessed_clean, de_word2ind, sos = True,  eos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the inference model does not necessaryly need to get input batches. we can just give it. the whole input\n",
    "# data, but the the batchsize has to be specified as the lenght of the input data.\n",
    "nmt_model_utils.reset_graph()\n",
    "\n",
    "nmt = NMT_Model.NMT(en_word2ind,\n",
    "                    use_gru = true,\n",
    "                    en_ind2word,\n",
    "                    de_word2ind,\n",
    "                    de_ind2word,\n",
    "                    './models/local_one/my_model',\n",
    "                    'INFER',\n",
    "                    num_layers_encoder = num_layers_encoder,\n",
    "                    num_layers_decoder = num_layers_decoder,\n",
    "                    batch_size = len(en_inds[:50]),\n",
    "                    keep_probability = 1.0,\n",
    "                    learning_rate = 0.0,\n",
    "                    beam_width = 0,\n",
    "                    rnn_size_encoder = rnn_size_encoder,\n",
    "                    rnn_size_decoder = rnn_size_decoder)\n",
    "\n",
    "nmt.build_graph()\n",
    "preds = nmt.infer(en_inds[:50], restore_path =  './models/local_one/my_model', targets = _de_inds[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show some of the created translations\n",
    "# Note: the way bleu score is probably not the perfect way to do it\n",
    "nmt_model_utils.sample_results(preds, en_ind2word, de_ind2word, en_word2ind, de_word2ind, _de_inds[:50], en_inds[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
